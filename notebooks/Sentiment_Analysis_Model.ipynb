{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6adbcf01",
   "metadata": {},
   "source": [
    "# Modelo de Análisis de Sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e62a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset # Import the Dataset class\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer # Import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c842d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/content/tweets_con_sentimiento.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpiar_tweet(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)  # URLs\n",
    "    texto = re.sub(r\"\\@\\w+|\\#\", \"\", texto)  # menciones y hashtags\n",
    "    texto = re.sub(r\"\\s+\", \" \", texto).strip()  # espacios múltiples\n",
    "    return texto\n",
    "\n",
    "df[\"text\"] = df[\"txt\"].apply(limpiar_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df[['text', 'sentiment']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ff00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MAPEAR ETIQUETAS\n",
    "label_list = df_train[\"sentiment\"].unique().tolist()  # ['neutral', 'negative', 'positive']\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "df_train[\"labels\"] = df_train[\"sentiment\"].map(label_to_id)\n",
    "\n",
    "# Cargar CSV\n",
    "\n",
    "# Separar 70/30\n",
    "train_df, val_df = train_test_split(df_train, test_size=0.3, random_state=42, stratify=df_train['labels'])\n",
    "\n",
    "# Convertir a Dataset de HuggingFace\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\", \"labels\"]])\n",
    "val_dataset = Dataset.from_pandas(val_df[[\"text\", \"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b0706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized = val_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_encoder_layers = len(model.roberta.encoder.layer)\n",
    "start_unfreeze_idx = max(0, num_encoder_layers - 3)\n",
    "for i in range(start_unfreeze_idx, num_encoder_layers):\n",
    "    for param in model.roberta.encoder.layer[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze the classification head.\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=[]   #\n",
    ")\n",
    "\n",
    "# Métricas personalizadas\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb79318",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8544d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(val_tokenized)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./modelo_sentimientos\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "output_dir = \"/content/drive/MyDrive/mi_modelo_sentimientos\"\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "print(f\"Modelo y tokenizador guardados en: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d6a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda el modelo entrenado\n",
    "trainer.save_model(\"./modelo\")\n",
    "\n",
    "# Guarda el tokenizador (opcional, pero recomendado)\n",
    "tokenizer.save_pretrained(\"./tokenizador\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d86b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('/content/modelo_sentimientos.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
